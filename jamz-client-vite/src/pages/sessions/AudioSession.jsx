import React, { useState, useEffect, useRef } from 'react';
import { useParams, useNavigate } from 'react-router-dom';
import io from 'socket.io-client';
import * as mediasoupClient from 'mediasoup-client';
import { 
  Container, 
  Box, 
  Typography, 
  Paper, 
  Button, 
  CircularProgress,
  AppBar,
  Toolbar,
  IconButton,
  Dialog,
  DialogTitle,
  DialogContent,
  DialogActions,
  Grid,
  Alert,
  Slider,
  Tooltip,
  Avatar,
  List,
  ListItem,
  ListItemText,
  ListItemAvatar,
  Divider,
  FormControlLabel,
  Switch,
  Chip
} from '@mui/material';
import { 
  ArrowBack as ArrowBackIcon,
  Mic as MicIcon,
  MicOff as MicOffIcon,
  VolumeUp as VolumeUpIcon,
  VolumeOff as VolumeOffIcon,
  VolumeDown as VolumeDownIcon,
  MusicNote as MusicNoteIcon,
  Group as GroupIcon,
  ExitToApp as LeaveIcon
} from '@mui/icons-material';
import api from '../../services/api';
import { useAuth } from '../../contexts/AuthContext';
import { useMusic } from '../../contexts/MusicContext';
import MusicUpload from '../../components/music/MusicUpload';
import MusicPlaylist from '../../components/music/MusicPlaylist';
import MusicPlayer from '../../components/music/MusicPlayer';
import MusicPlatformIntegration from '../../components/music/MusicPlatformIntegration';
import NativeAudio from '../../services/native-audio.service';

const AudioSession = () => {
  const { groupId } = useParams();
  const navigate = useNavigate();
  const { user } = useAuth();
  
  // Better mobile detection - avoid false positives from devtools responsive mode
  const isMobile = (() => {
    // Check for actual mobile device characteristics
    const hasTouchScreen = 'ontouchstart' in window || navigator.maxTouchPoints > 0;
    const isSmallScreen = window.innerWidth <= 768;
    const isIOSDevice = /iPhone|iPad|iPod/.test(navigator.userAgent);
    const isAndroid = /Android/.test(navigator.userAgent);

    // Additional check: if we're in a desktop browser but devtools responsive mode
    // is making it look small, check if the device actually has a small screen
    const actualScreenWidth = screen.width;
    const isActuallySmallDevice = actualScreenWidth <= 768;


    // Consider it mobile if it's actually a small device OR explicitly iOS/Android
    const result = isActuallySmallDevice || isIOSDevice || isAndroid;
    return result;
  })();

  // iOS Safari specifically requires user gesture for media access
  const isIOS = /iPhone|iPad|iPod/.test(navigator.userAgent) &&
                /Safari/.test(navigator.userAgent) &&
                !/Chrome|CriOS|FxiOS|Opera/.test(navigator.userAgent);

  // State for microphone initialization
  const [micInitialized, setMicInitialized] = useState(false);
  const [micInitializing, setMicInitializing] = useState(false);
  const [requiresUserGesture, setRequiresUserGesture] = useState(true); // Always require user gesture for simplicity
  
  // Session state
  const [session, setSession] = useState(null);
  const [group, setGroup] = useState(null);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState('');
  const [participants, setParticipants] = useState([]);
  
  // Music context - centralized state management
  const {
    currentTrack,
    playlist,
    isPlaying: musicIsPlaying,
    currentTime: musicCurrentTime,
    duration: musicDuration,
    volume: musicVolume,
    isController: isMusicController,
    isMusicEnabled,
    initializeSession: initializeMusicSession,
    play: musicPlay,
    pause: musicPause,
    seekTo: musicSeek,
    playNext: musicNext,
    playPrevious: musicPrevious,
    addTrack: musicAddTrack,
    removeTrack: musicRemoveTrack,
    loadAndPlay: musicLoadAndPlay,
    toggleMusic: toggleMusicFeature,
    takeControl: takeMusicControl,
    releaseControl: releaseMusicControl,
    changeVolume: changeMusicVolume
  } = useMusic();
  
  // Audio state
  const [localStream, setLocalStream] = useState(null);
  const [isJoined, setIsJoined] = useState(false);
  const [isMuted, setIsMuted] = useState(false);
  const [audioError, setAudioError] = useState(null);
  const [inputLevel, setInputLevel] = useState(0);
  const [inputVolume, setInputVolume] = useState(1.0);
  const [outputVolume, setOutputVolume] = useState(1.0);
  const [localAudioMonitoring, setLocalAudioMonitoring] = useState(false); // Disable by default for actual audio testing
  
  // Listener mute controls
  const [isMusicMuted, setIsMusicMuted] = useState(false);
  const [isVoiceMuted, setIsVoiceMuted] = useState(false);
  const [showIOSAudioPrompt, setShowIOSAudioPrompt] = useState(false);
  
  // Simplified Push-to-talk state
  const [isPushToTalkActive, setIsPushToTalkActive] = useState(false);
  const [micButtonPressStartTime, setMicButtonPressStartTime] = useState(null);
  const [isLongPress, setIsLongPress] = useState(false);
  const longPressTimerRef = useRef(null);
  
  // Music state
  const [openMusicDialog, setOpenMusicDialog] = useState(false);
  
  // Dialog state
  const [openLeaveDialog, setOpenLeaveDialog] = useState(false);
  const [leaveLoading, setLeaveLoading] = useState(false);
  
  // WebRTC state
  const [socket, setSocket] = useState(null);
  const [webrtcReady, setWebrtcReady] = useState(false);
  const [connecting, setConnecting] = useState(false);
  const [connected, setConnected] = useState(false);
  const [peerReady, setPeerReady] = useState(false); // Track if remote peer is ready
  
  // Device state
  const [audioDevices, setAudioDevices] = useState({ inputs: [], outputs: [] });
  const [selectedInputDevice, setSelectedInputDevice] = useState('');
  const [selectedOutputDevice, setSelectedOutputDevice] = useState('');
  
  // Refs
  const localStreamRef = useRef(null);
  const rtcConnectionRef = useRef(null);
  const audioPlayerRef = useRef(null);
  const signalingRef = useRef(null); // Add ref for signaling
  const sessionIdRef = useRef(null); // Store current session ID for WebRTC handlers
  const mediasoupDeviceRef = useRef(null); // Store mediasoup device
  const recvTransportRef = useRef(null); // Store receive transport for consuming
  const consumersRef = useRef(new Map()); // Store consumers
  
  // Initialize component
  useEffect(() => {
    let isMounted = true; // Track if component is still mounted
    
    const initializeComponent = async () => {
      try {
        console.log('AudioSession component mounting with groupId:', groupId);
        
        // If groupId is missing (bad route), bail out early and don't attempt
        // to auto-join or call backend APIs. This prevents POSTs with
        // `undefined` group IDs and avoids initializing WebRTC with
        // undefined group identifiers.
        if (!groupId) {
          console.warn('AudioSession mounted without groupId in route params');
          if (isMounted) {
            setError('Missing group ID in URL');
            setLoading(false);
          }
          return;
        }

        console.log('Group ID validated, proceeding with initialization');

        // Setup native audio session for better audio handling on native apps
        await NativeAudio.setupAudioSession();
        
        // Enable background audio on native apps
        if (NativeAudio.isNativeApp()) {
          await NativeAudio.enableBackgroundAudio();
        }

        // Only auto-initialize the microphone on non-iOS devices.
        // iOS Safari requires a user gesture; forcing auto-init there causes
        // the getUserMedia prompt to be blocked. For iOS we instead present
        // a visible button the user must tap to enable the microphone.
        if (!isIOS) {
          console.log('ðŸŽ¤ Non-iOS detected â€” setting up auto-init timeout...');
          setRequiresUserGesture(false);
          setTimeout(() => {
            console.log('ðŸŽ¤ Auto-init timeout triggered, calling initializeMicrophone...');
            initializeMicrophone().then(() => {
              console.log('ðŸŽ¤ Auto-init succeeded');
            }).catch(error => {
              console.log('ðŸŽ¤ Auto-init failed:', error.message, error.name);
              setAudioError(`Microphone access failed: ${error.message}. Please check your browser permissions and refresh the page.`);
            });
          }, 1000); // Small delay to ensure component is ready
        } else {
          console.log('ðŸŽ¤ iOS detected â€” skipping auto-init. Showing enable-microphone button.');
          setRequiresUserGesture(true);
        }
      } catch (error) {
        console.error('Error in component initialization:', error);
        if (isMounted) {
          setError('Initialization failed: ' + error.message);
        }
      }

      try {
        console.log('Calling fetchSessionDetails with groupId:', groupId);
        await fetchSessionDetails();
        console.log('Session details fetched successfully');
      } catch (error) {
        console.error('Error in fetchSessionDetails:', error);
        if (isMounted) {
          setError('Failed to fetch session details: ' + error.message);
          setLoading(false);
        }
      }
    };

    // Call the async initialization
    initializeComponent().catch(error => {
      console.error('Unhandled error in initializeComponent:', error);
      if (isMounted) {
        setError('Initialization failed: ' + error.message);
        setLoading(false);
      }
    });
    
    // Enumerate available devices
    navigator.mediaDevices?.enumerateDevices()
      .then(devices => {
        if (!isMounted) return;
        const inputs = devices.filter(device => device.kind === 'audioinput');
        const outputs = devices.filter(device => device.kind === 'audiooutput');
        setAudioDevices({ inputs, outputs });
        
        // Set defaults if available
        if (inputs.length > 0 && !selectedInputDevice) {
          setSelectedInputDevice(inputs[0].deviceId);
        }
        if (outputs.length > 0 && !selectedOutputDevice) {
          setSelectedOutputDevice(outputs[0].deviceId);
        }
      })
      .catch(error => {
        console.error('Error enumerating devices:', error);
        // Don't set error state for device enumeration failures
      });
    
    // Cleanup function for when component unmounts
    return () => {
      console.log('AudioSession component unmounting');
      isMounted = false;
      if (localStream) {
        localStream.getTracks().forEach(track => track.stop());
      }

      console.log('Session unmount occurred, leaving session.');

      leaveSession();
    };
  }, []);
  
  // Audio level monitoring
  useEffect(() => {
    if (localStream) {
      setupAudioLevelMonitoring(localStream);
      
      // If we have a peer connection but no tracks, add them now
      if (rtcConnectionRef.current) {
        const senders = rtcConnectionRef.current.getSenders();
        console.log('ðŸŽµ Current peer connection has', senders.length, 'senders');
        
        if (senders.length === 0) {
          console.log('ðŸŽµ Adding tracks to existing peer connection...');
          localStream.getTracks().forEach(track => {
            console.log('ðŸŽµ Adding track:', track.kind, track.id);
            rtcConnectionRef.current.addTrack(track, localStream);
          });
          
          // If signaling is connected and peer is ready, create offer now
          if (signalingRef.current && peerReady) {
            console.log('ðŸŽµ Tracks added - creating offer...');
            createAndSendOffer(signalingRef.current);
          }
        }
      }
    }
  }, [localStream, peerReady]);
  
  // Socket connection for music events
  useEffect(() => {
    if (socket) {
      socket.on('music_play', (data) => {
        setMusicState(prev => ({
          ...prev,
          currentTrack: data.trackData,
          isPlaying: true,
          position: data.position,
          serverTimestamp: data.timestamp
        }));
        
        if (audioPlayerRef.current) {
          audioPlayerRef.current.currentTime = data.position / 1000;
          audioPlayerRef.current.play().catch(err => console.error('Error playing audio:', err));
        }
      });
      
      socket.on('music_pause', (data) => {
        setMusicState(prev => ({
          ...prev,
          isPlaying: false,
          position: data.position
        }));
        
        if (audioPlayerRef.current) {
          audioPlayerRef.current.pause();
        }
      });
      
      socket.on('music_seek', (data) => {
        setMusicState(prev => ({
          ...prev,
          position: data.position
        }));
        
        if (audioPlayerRef.current) {
          audioPlayerRef.current.currentTime = data.position / 1000;
        }
      });
      
      return () => {
        socket.off('music_play');
        socket.off('music_pause');
        socket.off('music_seek');
      };
    }
  }, [socket]);
  
  // Update volume of remote audio elements when outputVolume changes
  useEffect(() => {
    const remoteAudios = document.getElementById('remote-audios');
    
    if (remoteAudios) {
      const audioElements = remoteAudios.querySelectorAll('audio');
      const usesDevice = NativeAudio.usesDeviceVolume();
      console.log('ðŸ”Š Updating voice volume:', outputVolume, 'for', audioElements.length, 'audio elements', 
                  usesDevice ? '(device volume only)' : '');
      
      audioElements.forEach((audio, index) => {
        NativeAudio.configureAudioElement(audio, outputVolume, audio.muted);
        console.log(`ðŸ”Š Audio element ${index}: volume=${audio.volume}, muted=${audio.muted}`);
      });
    } else {
      console.warn('ðŸ”Š Remote audios container not found');
    }
  }, [outputVolume]);
  
  // Store the previous volume before muting
  const previousMusicVolumeRef = useRef(1.0);
  
  // Update previousVolume ref whenever volume changes (but not when muted)
  useEffect(() => {
    if (!isMusicMuted && musicVolume > 0) {
      previousMusicVolumeRef.current = musicVolume;
    }
  }, [musicVolume, isMusicMuted]);
  
  // Apply music mute state
  useEffect(() => {
    if (isMusicMuted) {
      changeMusicVolume(0);
      console.log('ðŸŽµ Music muted (saved volume:', previousMusicVolumeRef.current, ')');
    } else if (musicVolume === 0) {
      // Only restore if current volume is 0 (was muted)
      const volumeToRestore = previousMusicVolumeRef.current > 0 ? previousMusicVolumeRef.current : 0.5;
      changeMusicVolume(volumeToRestore);
      console.log('ðŸŽµ Music unmuted (restored volume:', volumeToRestore, ')');
    }
  }, [isMusicMuted, changeMusicVolume]);
  
  // Apply voice mute state to all remote streams
  useEffect(() => {
    const remoteAudios = document.getElementById('remote-audios');
    
    if (remoteAudios) {
      const audioElements = remoteAudios.querySelectorAll('audio');
      console.log('ðŸ”‡ Setting voice mute:', isVoiceMuted, 'for', audioElements.length, 'audio elements');
      
      audioElements.forEach((audio, index) => {
        NativeAudio.configureAudioElement(audio, audio.volume, isVoiceMuted);
        console.log(`ðŸ”‡ Audio element ${index}: muted=${audio.muted}, volume=${audio.volume}`);
      });
    } else {
      console.warn('ðŸ”‡ Remote audios container not found');
    }
  }, [isVoiceMuted]);
  
  // Initialize microphone (called from user gesture for browsers that require it)
  const initializeMicrophone = async () => {
    console.log('ðŸŽ¤ Initializing microphone from user gesture...');
    setMicInitializing(true);
    setAudioError(null);

    try {
      console.log('ðŸŽ¤ navigator.mediaDevices available:', !!navigator.mediaDevices);
      console.log('ðŸŽ¤ getUserMedia available:', !!navigator.mediaDevices?.getUserMedia);
      console.log('ðŸŽ¤ webkitGetUserMedia available:', !!navigator.webkitGetUserMedia);
      console.log('ðŸŽ¤ mozGetUserMedia available:', !!navigator.mozGetUserMedia);
      console.log('ðŸŽ¤ Is secure context (HTTPS):', window.isSecureContext);
      console.log('ðŸŽ¤ Protocol:', window.location.protocol);
      console.log('ðŸŽ¤ Full navigator object keys:', Object.keys(navigator).filter(key => key.toLowerCase().includes('media') || key.toLowerCase().includes('webkit') || key.toLowerCase().includes('getuser')));
      console.log('ðŸŽ¤ User agent:', navigator.userAgent);
      console.log('ðŸŽ¤ Is mobile device:', /iPhone|iPad|iPod|Android/i.test(navigator.userAgent));
      console.log('ðŸŽ¤ Is iOS Safari:', /iPhone|iPad|iPod/.test(navigator.userAgent) && /Safari/.test(navigator.userAgent) && !/Chrome|CriOS|FxiOS|Opera/.test(navigator.userAgent));

      // Extract iOS version for better error messages
      const iOSVersion = navigator.userAgent.match(/OS (\d+)_?(\d+)?/);
      console.log('ðŸŽ¤ iOS version:', iOSVersion ? `${iOSVersion[1]}.${iOSVersion[2] || '0'}` : 'Not iOS');

      // Get the appropriate getUserMedia function for cross-browser compatibility
      let getUserMedia = null;

      if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
        getUserMedia = navigator.mediaDevices.getUserMedia.bind(navigator.mediaDevices);
        console.log('ðŸŽ¤ Using modern navigator.mediaDevices.getUserMedia');
      } else if (navigator.webkitGetUserMedia) {
        getUserMedia = navigator.webkitGetUserMedia.bind(navigator);
        console.log('ðŸŽ¤ Using legacy navigator.webkitGetUserMedia');
      } else if (navigator.mozGetUserMedia) {
        getUserMedia = navigator.mozGetUserMedia.bind(navigator);
        console.log('ðŸŽ¤ Using legacy navigator.mozGetUserMedia');
      } else if (navigator.getUserMedia) {
        getUserMedia = navigator.getUserMedia.bind(navigator);
        console.log('ðŸŽ¤ Using fallback navigator.getUserMedia');
      }

      if (!getUserMedia) {
        console.error('ðŸŽ¤ No getUserMedia API found!');
        console.error('ðŸŽ¤ navigator object:', navigator);
        throw new Error('getUserMedia is not supported in this browser');
      }

      // Create promise-based wrapper for older callback-based APIs
      const getUserMediaPromise = (constraints) => {
        return new Promise((resolve, reject) => {
          // Remove timeout for debugging - let it hang if dialog not shown
          // const timeout = setTimeout(() => {
          //   reject(new Error('Microphone access timed out. Please check your browser settings and try again.'));
          // }, 5000); // 5 second timeout

          try {
            // For modern API: navigator.mediaDevices.getUserMedia(constraints).then(resolve).catch(reject)
            if (getUserMedia === navigator.mediaDevices.getUserMedia.bind(navigator.mediaDevices)) {
              getUserMedia(constraints)
                .then((stream) => {
                  // clearTimeout(timeout);
                  resolve(stream);
                })
                .catch((error) => {
                  // clearTimeout(timeout);
                  reject(error);
                });
            }
            // For legacy APIs: navigator.webkitGetUserMedia(constraints, successCallback, errorCallback)
            else {
              getUserMedia(constraints,
                (stream) => {
                  // clearTimeout(timeout);
                  resolve(stream);
                },
                (error) => {
                  // clearTimeout(timeout);
                  reject(error);
                }
              );
            }
          } catch (err) {
            // clearTimeout(timeout);
            console.error('ðŸŽ¤ Error in getUserMediaPromise wrapper:', err);
            reject(err);
          }
        });
      };

      console.log('ðŸŽ¤ Starting getUserMedia call...');
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: true,
        video: false
      });
      console.log('ðŸŽ¤ getUserMediaPromise resolved successfully');

      console.log('âœ… Microphone access granted!');
      console.log('âœ… Stream tracks:', stream.getTracks().map(t => ({ kind: t.kind, enabled: t.enabled, readyState: t.readyState })));
      console.log('âœ… Audio tracks count:', stream.getAudioTracks().length);
      console.log('âœ… Video tracks count:', stream.getVideoTracks().length);

      setLocalStream(stream);
      localStreamRef.current = stream;
      setMicInitialized(true);
      setRequiresUserGesture(false); // Clear the flag if it was set
      setIsJoined(true);
      console.log('ðŸŽµ Microphone initialized successfully');

      // Set up audio level monitoring
      setupAudioLevelMonitoring(stream);

    } catch (error) {
      console.error('âŒ Error accessing microphone:', error.message);

      const errorMessage = error.name === 'NotAllowedError'
        ? 'Microphone access denied. Please allow microphone permissions and try again.'
        : error.name === 'NotFoundError'
        ? 'No microphone found. Please connect a microphone and try again.'
        : error.name === 'NotSupportedError'
        ? 'Microphone access not supported in this browser. Please use a modern browser.'
        : error.message === 'getUserMedia is not supported in this browser'
        ? `Your browser does not support microphone access. Current protocol: ${window.location.protocol}. For iPhone testing, you need HTTPS. Use ngrok: 'ngrok http 5173' then access via the HTTPS ngrok URL.`
        : error.message === 'Microphone access timed out. Please check your browser settings and try again.'
        ? 'Microphone access timed out. This often happens on mobile devices. Please ensure you are using HTTPS and try again.'
        : `Could not access microphone: ${error.message}`;

      setAudioError(errorMessage);
      setMicInitialized(false);
      setIsJoined(false);
      throw error; // Re-throw so the caller knows it failed
    } finally {
      setMicInitializing(false);
    }
  };  // Leave audio session
  const handleLeaveAudio = async () => {
    console.log('ðŸšª Leaving audio session...');
    
    // Stop local stream
    if (localStream) {
      localStream.getTracks().forEach(track => track.stop());
      setLocalStream(null);
    }
    
    // Clean up state
    setIsJoined(false);
    setIsMuted(false);
    setIsPushToTalkActive(false);
    setMicInitialized(false);
    setAudioError(null);
    
    // Leave session and clean up connections
    await leaveSession();
    
    // Navigate back to the group detail page (where users came from)
    console.log('ðŸšª Navigating to group detail page...');
    navigate(`/groups/${groupId}`, { replace: true });
  };
  
  // Toggle microphone mute
  const toggleMute = () => {
    if (localStream) {
      const newMuteState = !isMuted;
      console.log('ðŸ”‡ Toggle mute:', { currentMuted: isMuted, newMuteState });
      localStream.getAudioTracks().forEach(track => {
        console.log('ðŸ”‡ Setting track enabled:', track.kind, 'from', track.enabled, 'to', !newMuteState);
        track.enabled = !newMuteState;
      });
      setIsMuted(newMuteState);
      console.log('ðŸ”‡ Mute state updated to:', newMuteState);
    } else {
      console.warn('ðŸ”‡ Cannot toggle mute: no local stream available');
    }
  };
  
  // Toggle music mute for listeners
  const toggleMusicMute = () => {
    const newMuteState = !isMusicMuted;
    setIsMusicMuted(newMuteState);
    console.log('ðŸŽµ Music mute toggled:', newMuteState);
  };
  
  // Toggle voice mute for listeners
  const toggleVoiceMute = () => {
    const newMuteState = !isVoiceMuted;
    setIsVoiceMuted(newMuteState);
    console.log('ðŸ”Š Voice mute toggled:', newMuteState);
  };
  
  // New unified mic control handlers
  const handleMicButtonMouseDown = () => {
    // Record the time when the button was pressed
    const pressStartTime = Date.now();
    setMicButtonPressStartTime(pressStartTime);
    
    // Set a timer to detect long press (>1 second)
    longPressTimerRef.current = setTimeout(() => {
      // Only activate push-to-talk if the mic is currently muted
      if (isMuted) {
        setIsLongPress(true);
        handlePushToTalkStart();
      }
    }, 1000); // 1 second threshold for long press
  };

  const handleMicButtonMouseUp = () => {
    // Clear the long press timer
    if (longPressTimerRef.current) {
      clearTimeout(longPressTimerRef.current);
    }
    
    const pressDuration = Date.now() - (micButtonPressStartTime || Date.now());
    
    // If it was a long press and push-to-talk is active, deactivate it
    if (isLongPress && isPushToTalkActive) {
      handlePushToTalkEnd();
      setIsLongPress(false);
    } 
    // If it was a short press (click), toggle mute/unmute
    else if (pressDuration < 1000) {
      toggleMute();
    }
    
    setMicButtonPressStartTime(null);
  };

  // Handle mouse leave to prevent stuck states
  const handleMicButtonMouseLeave = () => {
    // If button is pressed and mouse leaves, treat as mouse up
    if (micButtonPressStartTime !== null) {
      handleMicButtonMouseUp();
    }
  };

  // Push-to-talk functionality
  const handlePushToTalkStart = () => {
    if (isMuted && localStream) {
      setIsPushToTalkActive(true);
      localStream.getAudioTracks().forEach(track => {
        track.enabled = true;
      });
    }
  };

  const handlePushToTalkEnd = () => {
    if (isPushToTalkActive && localStream) {
      setIsPushToTalkActive(false);
      // Only mute if the mic was muted before push-to-talk
      if (isMuted) {
        localStream.getAudioTracks().forEach(track => {
          track.enabled = false;
        });
      }
    }
  };
  
  // Audio level monitoring
  const setupAudioLevelMonitoring = (stream) => {
    const audioContext = new (window.AudioContext || window.webkitAudioContext)();
    const analyser = audioContext.createAnalyser();
    const source = audioContext.createMediaStreamSource(stream);
    
    analyser.fftSize = 256;
    source.connect(analyser);
    
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    
    let lastUpdateTime = 0;
    const updateInterval = 100; // Update every 100ms instead of every frame
    
    const updateMeter = (timestamp) => {
      if (timestamp - lastUpdateTime >= updateInterval) {
        analyser.getByteFrequencyData(dataArray);
        
        let sum = 0;
        for (let i = 0; i < bufferLength; i++) {
          sum += dataArray[i];
        }
        
        const average = sum / bufferLength;
        setInputLevel(average / 255); // Normalize to 0-1
        lastUpdateTime = timestamp;
      }
      
      requestAnimationFrame(updateMeter);
    };
    
    requestAnimationFrame(updateMeter);
    
    return () => {
      source.disconnect();
      audioContext.close();
    };
  };
  
  // Fetch session details from API
  const fetchSessionDetails = async () => {
    console.log('ðŸš€ fetchSessionDetails start', { groupId, user, groupIdType: typeof groupId, userType: typeof user });
    setLoading(true);

    if (!groupId || !user) {
      console.error('Missing required parameters:', { groupId, user });
      setError('Missing group ID or user information');
      setLoading(false);
      return;
    }

    let sessionData = null;

    // Helper to log axios error details
    const logAxiosError = (prefix, err) => {
      try {
        console.error(prefix, err && err.message ? err.message : err);
        if (err && err.response) {
          console.error(prefix + ' - response status:', err.response.status);
          console.error(prefix + ' - response data:', err.response.data);
        }
      } catch (e) {
        console.error('Failed to log axios error', e);
      }
    };

    // Try to GET existing session
    try {
      console.log('Checking for existing audio session for group:', groupId);
      // Avoid sending custom headers here (Cache-Control/Pragma/Expires)
      // since they trigger CORS preflight checks on some client origins
      // (and iOS/remote devices will fail if backend doesn't allow these).
      // Use a cache-busting query param if needed instead.
      const response = await api.get(`/audio/sessions/group/${groupId}`);
      console.log('GET session response status:', response.status);
      console.log('GET session response data:', response.data);
      
      if (response.status === 200 && response.data && response.data.session) {
        sessionData = response.data.session;
        console.log('Found existing session:', sessionData.id);
        console.log('ðŸ‘¥ [DEBUG] Session participants from backend:', JSON.stringify(sessionData.participants, null, 2));
      } else {
        console.log('No existing session found (status:', response.status, ', data:', response.data, '), will create new one');
      }
    } catch (err) {
      logAxiosError('GET /audio/sessions/group failed', err);
      console.log('GET failed, will attempt to create new session. Error details:', err.response?.status, err.response?.data);
      // proceed to create
    }

    // Create session if missing
    if (!sessionData) {
      try {
        console.log('Creating new audio session for group:', groupId);
        const createResponse = await api.post('/audio/sessions', {
          group_id: groupId,
          session_type: 'voice_with_music',
          device_type: 'web'
        });

        if (createResponse && createResponse.data && createResponse.data.session) {
          sessionData = createResponse.data.session;
          console.log('Session created successfully:', sessionData.id);
        } else {
          console.error('âŒ CRITICAL: Create session returned unexpected body', createResponse && createResponse.data);
          // DO NOT use fallback - this breaks music uploads and other backend operations
          throw new Error('Backend returned invalid session data. Session creation completely failed.');
        }
      } catch (err) {
        logAxiosError('POST /audio/sessions failed', err);
        console.error('âŒ CRITICAL: Failed to create audio session on backend');
        console.error('âŒ CRITICAL: This will prevent music uploads and other features from working');
        console.error('âŒ CRITICAL: Error details:', {
          message: err.message,
          status: err.response?.status,
          data: err.response?.data
        });
        
        // Show user-friendly error
        setError(`Failed to create audio session: ${err.response?.data?.message || err.message}. Please check if the backend server is running.`);
        setLoading(false);
        return; // Stop here - don't try to continue with invalid session
      }
    }

    // Ensure we have valid session data
    console.log('ðŸ” DEBUG: About to check sessionData validity.');
    console.log('ðŸ” DEBUG: sessionData type:', typeof sessionData);
    console.log('ðŸ” DEBUG: sessionData value:', JSON.stringify(sessionData, null, 2));
    console.log('ðŸ” DEBUG: sessionData.id exists:', !!sessionData?.id);
    console.log('ðŸ” DEBUG: sessionData.id value:', sessionData?.id);
    console.log('ðŸ” DEBUG: sessionData._id exists:', !!sessionData?._id);
    console.log('ðŸ” DEBUG: sessionData._id value:', sessionData?._id);
    console.log('ðŸ” DEBUG: Boolean check result:', !sessionData || (!sessionData.id && !sessionData._id));
    if (!sessionData || (!sessionData.id && !sessionData._id)) {
      console.error('âŒ ERROR: No valid session data available after all attempts.');
      console.error('âŒ ERROR: sessionData details:', {
        exists: !!sessionData,
        type: typeof sessionData,
        value: sessionData,
        hasId: !!(sessionData && sessionData.id),
        idValue: sessionData?.id,
        hasUnderscoreId: !!(sessionData && sessionData._id),
        underscoreIdValue: sessionData?._id
      });
      console.error('Session creation failed completely. This should not happen with fallback logic.');
      setError('Unable to create or retrieve audio session. Please try refreshing the page.');
      setLoading(false);
      return;
    }

    // Normalize session data - map _id to id if needed
    if (sessionData._id && !sessionData.id) {
      sessionData.id = sessionData._id;
      console.log('ðŸ”„ Mapped _id to id for session:', sessionData.id);
    }

    console.log('Successfully obtained session data:', sessionData.id);

    // Join the session as a participant (best-effort)
    if (sessionData && sessionData.id) {
      try {
        console.log('Joining audio session:', sessionData.id);
        const joinResponse = await api.post(`/audio/sessions/${sessionData.id}/join`, {
          display_name: user.first_name || user.username || 'User',
          device_type: 'web'
        });

        if (!(joinResponse && joinResponse.data && joinResponse.data.success)) {
          console.warn('Join response incomplete or false', joinResponse && joinResponse.data);
        } else {
          console.log('Successfully joined audio session');
        }
      } catch (err) {
        logAxiosError('POST join session failed', err);
        // Don't fail completely if join fails - user can still connect signaling
        console.warn('Failed to join session, but continuing:', err.response?.data?.message || err.message);
      }
    } else {
      console.error('No valid session data to join');
      setError('No valid session data available');
      setLoading(false);
      return;
    }

    // Apply session state
    try {
      setSession(sessionData);

      // Fetch group details for display
      try {
        console.log('ðŸ“‹ Fetching group details for:', sessionData.group_id);
        const groupResponse = await api.get(`/groups/${sessionData.group_id}`);
        if (groupResponse.data && groupResponse.data.group) {
          setGroup(groupResponse.data.group);
          console.log('ðŸ“‹ Group loaded:', groupResponse.data.group.name);
        }
      } catch (err) {
        console.warn('Failed to fetch group details:', err.message);
        // Continue without group name
      }

      console.log('ðŸ‘¥ [DEBUG] Processing participants - raw data:', sessionData.participants);
      console.log('ðŸ‘¥ [DEBUG] Participants is array?', Array.isArray(sessionData.participants));
      console.log('ðŸ‘¥ [DEBUG] Participants length:', sessionData.participants?.length);
      
      if (sessionData.participants && Array.isArray(sessionData.participants)) {
        // Filter out participants who have left and ensure valid participant objects
        // MongoDB uses user_id and socket_id (with underscores), need to map to camelCase
        const beforeFilter = sessionData.participants.length;
        const validParticipants = sessionData.participants
          .filter(p => {
            const isValid = p && !p.left_at;
            if (!isValid) console.log('ðŸ‘¥ [DEBUG] Filtering out participant:', p);
            return isValid;
          })
          .map(p => {
            const mapped = {
              id: p.user_id || p.id || null,
              socketId: p.socket_id || p.socketId || null,
              display_name: p.display_name || p.first_name || 'Unknown',
              isMuted: p.isMuted || false,
              profile_image_url: p.profile_image_url || null,
              first_name: p.first_name || null,
              last_name: p.last_name || null,
              isMe: false // Will be set to true when socket connects
            };
            console.log('ðŸ‘¥ [DEBUG] Mapped participant:', p, 'â†’', mapped);
            return mapped;
          });
        console.log(`ðŸ‘¥ [DEBUG] Filtered ${beforeFilter} â†’ ${validParticipants.length} participants`);
        console.log('ðŸ‘¥ Loaded participants from session data:', validParticipants.map(p => `${p.display_name} (${p.id})`).join(', '));
        setParticipants(validParticipants);
      } else {
        console.log('ðŸ‘¥ [DEBUG] No valid participants array in session data');
        setParticipants([]);
      }

      // Initialize music session with centralized context
      console.log('ðŸŽµ Initializing music session context - sessionId:', sessionData.id, 'groupId:', groupId);
      initializeMusicSession(sessionData.id, groupId);

      // AUTO-CONNECT: Automatically connect signaling when session loads
      // This ensures participants show up and remote audio works without requiring button click
      console.log('ðŸš€ AUTO-CONNECT: Setting up signaling automatically...');
      try {
        setConnecting(true);
        const audioSessionId = sessionData.id;
        const s = setupSignaling(audioSessionId);
        console.log('ðŸ“¡ AUTO-CONNECT: Signaling setup initiated for session:', audioSessionId, 'groupId:', groupId);
        
        // When socket connects, initialize WebRTC automatically
        s.on('connect', () => {
          console.log('ðŸŽ¯ AUTO-CONNECT: Socket connected, initializing WebRTC...');
          initializeWebRTC(audioSessionId);
        });
      } catch (err) {
        console.error('âŒ AUTO-CONNECT: Error setting up signaling:', err);
        setConnecting(false);
        // Don't set hard error - user can still manually connect via button
        console.warn('AUTO-CONNECT failed, user can manually connect via button');
      }
    } catch (err) {
      console.error('Error applying session state:', err);
      setError('Failed to initialize session state');
    } finally {
      setLoading(false);
    }
  };
  
  // Initialize WebRTC connection
  const initializeWebRTC = async (sessionId) => {
    console.log('ðŸŽ¥ Initializing WebRTC for session:', sessionId);

    // Set up signaling first
    // Note: setupSignaling now is called explicitly by user action (Connect)
    // so initializeWebRTC will only prepare the RTCPeerConnection when
    // signaling is available.
    const signaling = signalingRef.current;
    if (!signaling) {
      console.warn('ðŸŽ¥ Signaling not connected yet, skipping WebRTC init');
      return;
    }

    // Mediasoup enabled on DigitalOcean for SFU architecture
    // Supports 50-100+ users vs 4-6 peer-to-peer limit
    const useMediasoup = true;

    if (useMediasoup) {
      // Try mediasoup first, fall back to peer-to-peer if it fails
      try {
        console.log('ðŸŽ¥ Attempting mediasoup publish...');
        await startMediasoupPublish(signaling);
        console.log('âœ… Mediasoup publish successful');
        return;
      } catch (error) {
        console.warn('âš ï¸ Mediasoup publish failed, falling back to peer-to-peer:', error.message);
      }
    } else {
      console.log('ðŸŽ¥ Using peer-to-peer mode (mediasoup disabled)');
    }

    // Fallback to peer-to-peer WebRTC
    console.log('ðŸŽ¥ Setting up peer connection (fallback mode)...');
    await setupPeerConnection(signaling);

    console.log('ðŸŽ¥ WebRTC initialization complete');
    
    // If we have a local stream and a peer is ready, create an offer
    if (localStreamRef.current && peerReady) {
      console.log('ðŸŽ¥ Local stream available and peer ready, creating offer...');
      await createAndSendOffer(signaling);
    }
  };  // Set up signaling for WebRTC
  const setupSignaling = (sessionId) => {
    // Store sessionId in ref for use in WebRTC event handlers
    sessionIdRef.current = sessionId;
    
    // Determine the socket URL based on environment
    // In development: use Vite dev server origin (proxies to backend)
    // In production: use VITE_BACKEND_URL if set, otherwise current origin
    const isDevelopment = import.meta.env.MODE === 'development';
    const socketUrl = isDevelopment 
      ? window.location.origin  // Dev: use Vite proxy
      : (import.meta.env.VITE_BACKEND_URL || window.location.origin); // Prod: use backend URL

    console.log('ðŸ”Œ Setting up Socket.IO signaling connection...');
    console.log('ðŸ”Œ Mode:', import.meta.env.MODE);
    console.log('ðŸ”Œ Socket URL:', socketUrl);
    console.log('ðŸ”Œ Environment variables:', {
      VITE_WS_URL: import.meta.env.VITE_WS_URL,
      VITE_BACKEND_URL: import.meta.env.VITE_BACKEND_URL,
      VITE_API_BASE: import.meta.env.VITE_API_BASE,
      window_location: window.location.origin,
      computed_socketUrl: socketUrl,
      isDevelopment
    });

    const socket = io(socketUrl, {
      autoConnect: false, // CRITICAL: Prevent auto-connection to register handlers first
      transports: ['websocket', 'polling'], // Try websocket first, fallback to polling
      timeout: 10000, // Increase timeout to 10 seconds
      forceNew: true, // Force new connection
      reconnection: true,
      reconnectionAttempts: 5,
      reconnectionDelay: 1000,
      // Explicitly set the path
      path: '/socket.io'
    });

    console.log('ðŸ‘¥ [Participants] Registering all socket event handlers before connection...');

    socket.on('connect', () => {
      console.log('âœ… Socket.IO connection established successfully');
      console.log('âœ… Socket transport used:', socket.io.engine.transport.name);
      console.log('âœ… Socket ID:', socket.id);
      setConnecting(false);
      setConnected(true);

      // Join the audio session room with user info
      // Note: sessionId here refers to the audio session document ID, NOT the groupId
      const displayName = user?.first_name || user?.username || 'User';
      const userId = user?.id || user?.user_id;
      
      socket.emit('join-audio-session', { 
        sessionId,
        userId: userId,
        display_name: displayName
      });
      console.log(`ðŸ“¡ Joined audio session room: ${sessionId} (groupId: ${groupId}) as ${displayName}`);
      
      // Add current user to participants list or update existing entry
      setParticipants(prev => {
        const me = {
          id: userId,
          socketId: socket.id,
          display_name: displayName,
          isMuted: false,
          isMe: true,
          profile_image_url: user?.profile_image_url || null,
          first_name: user?.first_name || null,
          last_name: user?.last_name || null
        };
        
        // Check if I already exist in the list (from initial load)
        const myIndex = prev.findIndex(p => p && p.id === userId);
        if (myIndex >= 0) {
          // Update my entry with socket info and mark as me
          const updated = [...prev];
          updated[myIndex] = { ...updated[myIndex], ...me };
          console.log('ðŸ‘¥ Updated my participant entry with socket ID');
          return updated;
        }
        
        // Not in list yet, add me
        console.log('ðŸ‘¥ Added myself to participant list');
        return [me, ...prev];
      });

      // Send ready signal to let others know we're here
      socket.emit('webrtc-ready', { sessionId });
      console.log('ðŸš€ Sent webrtc-ready signal');
    });

    socket.on('connect_error', (error) => {
      console.error('âŒ Socket.IO connection failed:', error);
      console.error('âŒ Error details:', {
        message: error.message,
        description: error.description,
        context: error.context,
        type: error.type,
        code: error.code
      });
      setConnecting(false);
      setConnected(false);
      setError(`Failed to connect to signaling server: ${error.message}`);
    });

    socket.on('connect_timeout', () => {
      console.error('â° Socket.IO connection timeout');
      setConnecting(false);
      setConnected(false);
      setError('Connection to signaling server timed out. Is the backend server running?');
    });

    socket.on('disconnect', (reason) => {
      console.log('ðŸ”Œ Socket.IO connection closed:', reason);
      setConnected(false);
      setConnecting(false);
      if (reason === 'io server disconnect') {
        setError('Disconnected by server. Please try reconnecting.');
      }
    });

    // WebRTC signaling events
    socket.on('webrtc-offer', async (data) => {
      console.log('ðŸ“¨ Received webrtc-offer:', data);
      await handleOffer(data, socket);
    });

    socket.on('webrtc-answer', async (data) => {
      console.log('ðŸ“¨ Received webrtc-answer:', data);
      await handleAnswer(data, socket);
    });

    socket.on('webrtc-candidate', async (data) => {
      console.log('ðŸ“¨ Received webrtc-candidate:', data);
      await handleCandidate(data, socket);
    });

    socket.on('webrtc-ready', (data) => {
      console.log('ðŸ“¨ Received webrtc-ready from another participant:', data);
      setPeerReady(true);
      
      // Another participant is ready, initiate connection if we're not already connected
      // Only create offer if we have a peer connection AND local stream
      if (rtcConnectionRef.current && localStreamRef.current) {
        const connectionState = rtcConnectionRef.current.connectionState;
        console.log('ðŸ” Connection state:', connectionState);
        
        if (connectionState === 'new' || connectionState === 'closed') {
          console.log('ðŸš€ Creating and sending offer...');
          createAndSendOffer(socket);
        } else {
          console.log('â­ï¸ Connection already in progress or established:', connectionState);
        }
      } else {
        console.log('â¸ï¸ Waiting for local stream before creating offer');
      }
    });

    // Participant presence events
    socket.on('participant-joined', (data) => {
      console.log('ðŸ‘¥ Participant joined via socket:', data);
      if (data && data.socketId) {
        setParticipants(prev => {
          // Check if already exists
          const exists = prev.some(p => p && p.socketId === data.socketId);
          if (exists) {
            console.log('ðŸ‘¥ Participant already in list, skipping');
            return prev;
          }
          
          const newParticipant = {
            id: data.userId || null,
            socketId: data.socketId,
            display_name: data.display_name || 'User',
            isMuted: data.isMuted || false,
            profile_image_url: data.profile_image_url || null,
            first_name: data.first_name || null,
            last_name: data.last_name || null
          };
          
          console.log('ðŸ‘¥ Adding participant:', newParticipant);
          return [...prev, newParticipant];
        });
      }
    });

    socket.on('participant-left', (data) => {
      console.log('ðŸ‘¥ Participant left via socket:', data);
      if (data && data.socketId) {
        setParticipants(prev => {
          const filtered = prev.filter(p => p && p.socketId !== data.socketId);
          console.log('ðŸ‘¥ Removed participant, remaining:', filtered.length);
          return filtered;
        });
      }
    });
    
    socket.on('current-participants', (data) => {
      console.log('ðŸ‘¥ ========================================');
      console.log('ðŸ‘¥ [Participants] CURRENT PARTICIPANTS RECEIVED');
      console.log('ðŸ‘¥ Received current participants:', JSON.stringify(data.participants, null, 2));
      console.log('ðŸ‘¥ Number of participants:', data.participants?.length || 0);
      if (data.participants && Array.isArray(data.participants)) {
        setParticipants(prev => {
          // Keep current user (marked with isMe)
          const me = prev.find(p => p && p.isMe);
          console.log('ðŸ‘¥ Current user in list:', me ? `${me.display_name} (${me.id})` : 'not found');
          
          // Add all other participants with full user details
          const others = data.participants.map(p => ({
            id: p.userId,
            socketId: p.socketId,
            display_name: p.display_name || 'User',
            isMuted: false,
            isMe: false,
            profile_image_url: p.profile_image_url || null,
            first_name: p.first_name || null,
            last_name: p.last_name || null
          }));
          
          console.log('ðŸ‘¥ Other participants:', others.map(p => `${p.display_name} (${p.id})`).join(', '));
          const finalList = me ? [me, ...others] : others;
          console.log('ðŸ‘¥ Final participants list count:', finalList.length);
          console.log('ðŸ‘¥ ========================================');
          
          return finalList;
        });
      }
    });
    
    socket.on('disconnect', () => {
      console.log('ðŸ‘¥ Socket disconnected, clearing remote participants');
      // Keep only the current user in the list when disconnected
      setParticipants(prev => prev.filter(p => p && p.isMe));
    });

    console.log('ðŸ‘¥ [Participants] All event handlers registered, now connecting socket...');

    // Store the socket in state and ref
    setSocket(socket);
    signalingRef.current = socket;

    // Connect the socket AFTER all handlers are registered
    socket.connect();
    console.log('ðŸ‘¥ [Participants] Socket connection initiated');

    return socket;
  };
  
  // Handle signaling messages
  const handleSignalingMessage = async (message, signaling) => {
    console.log('Received signaling message:', message.type);

    try {
      switch (message.type) {
        case 'offer':
          await handleOffer(message, signaling);
          break;
        case 'answer':
          await handleAnswer(message, signaling);
          break;
        case 'candidate':
          await handleCandidate(message, signaling);
          break;
        case 'participant_joined':
          handleParticipantJoined(message.participant);
          break;
        case 'participant_left':
          handleParticipantLeft(message.participant_id);
          break;
        case 'ready':
          // Another participant is ready, initiate connection if we're not already connected
          if (rtcConnectionRef.current && rtcConnectionRef.current.connectionState === 'new') {
            await createAndSendOffer(signaling);
          }
          break;
        default:
          console.log('Unknown message type:', message.type);
      }
    } catch (error) {
      console.error('Error handling signaling message:', error);
    }
  };
  
  // Set up peer connection
  const setupPeerConnection = async (signaling) => {
    console.log('ðŸ”— Setting up peer connection...');

    const configuration = {
      iceServers: [
        { urls: 'stun:stun.l.google.com:19302' },
        { urls: 'stun:stun1.l.google.com:19302' },
        // Add TURN servers for production if needed
        // { urls: 'turn:your-turn-server.com', username: 'user', credential: 'pass' }
      ]
    };

    const peerConnection = new RTCPeerConnection(configuration);
    rtcConnectionRef.current = peerConnection;
    console.log('ðŸ”— RTCPeerConnection created');

    // Add local stream to peer connection when available
    const streamToAdd = localStreamRef.current || localStream;
    if (streamToAdd) {
      console.log('ðŸ”— Adding local stream to peer connection');
      streamToAdd.getTracks().forEach(track => {
        console.log('ðŸ”— Adding track:', track.kind, track.id, 'enabled:', track.enabled, 'readyState:', track.readyState);
        peerConnection.addTrack(track, streamToAdd);
      });
    } else {
      console.warn('âš ï¸ No local stream available yet - peer connection created without tracks');
      console.warn('âš ï¸ Tracks will need to be added manually when stream becomes available');
    }

    // Handle ICE candidates
    peerConnection.onicecandidate = (event) => {
      if (event.candidate) {
        console.log('ðŸ§Š Sending ICE candidate');
        signaling.emit('webrtc-candidate', {
          candidate: event.candidate,
          sessionId: sessionIdRef.current
        });
      }
    };

    // Handle connection state changes
    peerConnection.onconnectionstatechange = () => {
      console.log('ðŸ”— Connection state changed:', peerConnection.connectionState);
      console.log('ðŸ”— Full connection state details:', {
        connectionState: peerConnection.connectionState,
        iceConnectionState: peerConnection.iceConnectionState,
        iceGatheringState: peerConnection.iceGatheringState,
        signalingState: peerConnection.signalingState
      });
      setConnected(peerConnection.connectionState === 'connected');
      setConnecting(peerConnection.connectionState === 'connecting');
    };

    // Handle ICE connection state changes
    peerConnection.oniceconnectionstatechange = () => {
      console.log('ðŸ§Š ICE connection state:', peerConnection.iceConnectionState);
      console.log('ðŸ§Š Full ICE state details:', {
        iceConnectionState: peerConnection.iceConnectionState,
        iceGatheringState: peerConnection.iceGatheringState,
        signalingState: peerConnection.signalingState
      });
    };

    // Handle remote stream
    peerConnection.ontrack = (event) => {
      console.log('ðŸŽµ ========== REMOTE TRACK RECEIVED ==========');
      console.log('ðŸŽµ Track kind:', event.track.kind);
      console.log('ðŸŽµ Track id:', event.track.id);
      console.log('ðŸŽµ Track enabled:', event.track.enabled);
      console.log('ðŸŽµ Track readyState:', event.track.readyState);
      console.log('ðŸŽµ Track muted:', event.track.muted);
      console.log('ðŸŽµ Number of streams:', event.streams.length);
      if (event.streams.length > 0) {
        console.log('ðŸŽµ Stream id:', event.streams[0].id);
        console.log('ðŸŽµ Stream tracks:', event.streams[0].getTracks().map(t => ({ kind: t.kind, id: t.id, enabled: t.enabled, readyState: t.readyState })));
      }
      console.log('ðŸŽµ ==========================================');
      
      if (event.track.kind === 'audio') {
        console.log('ðŸŽµ Setting up remote audio stream');
        handleRemoteAudioStream(event.streams[0]);
      }
    };

    // Store signaling reference
    signalingRef.current = signaling;

    return peerConnection;
  };
  
  // Handle remote audio stream
  const handleRemoteAudioStream = (stream) => {
    console.log('ðŸŽµ Handling remote audio stream');
    console.log('ðŸŽµ Remote stream tracks:', stream.getTracks().map(t => ({ kind: t.kind, id: t.id, enabled: t.enabled, readyState: t.readyState })));
    console.log('ðŸŽµ User agent:', navigator.userAgent);
    console.log('ðŸŽµ Platform:', NativeAudio.getPlatform(), 'Native:', NativeAudio.isNativeApp());

    // Create audio element for remote stream
    const audioElement = new Audio();
    audioElement.srcObject = stream;
    audioElement.autoplay = true;
    
    // Configure audio element for platform (iOS/Android/Desktop)
    NativeAudio.configureAudioElement(audioElement, outputVolume, isVoiceMuted);

    // Add event listeners to debug audio playback
    audioElement.onloadedmetadata = () => console.log('ðŸŽµ Audio element loaded metadata');
    audioElement.oncanplay = () => console.log('ðŸŽµ Audio element can play');
    audioElement.onplay = () => console.log('ðŸŽµ Audio element started playing');
    audioElement.onpause = () => console.log('ðŸŽµ Audio element paused');
    audioElement.onerror = (e) => console.error('ðŸŽµ Audio element error:', e);
    audioElement.onended = () => console.log('ðŸŽµ Audio element ended');

    // For iOS: Resume AudioContext if suspended (required for audio playback)
    if (isIOS && typeof window.AudioContext !== 'undefined') {
      const resumeAudioContext = async () => {
        try {
          const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
          if (audioCtx.state === 'suspended') {
            console.log('ðŸŽµ iOS AudioContext suspended, resuming...');
            await audioCtx.resume();
            console.log('ðŸŽµ iOS AudioContext resumed:', audioCtx.state);
          }
        } catch (err) {
          console.warn('ðŸŽµ Failed to resume AudioContext:', err);
        }
      };
      resumeAudioContext();
    }

    // Try to play immediately
    const attemptPlay = () => {
      console.log('ðŸŽµ Attempting to play audio element...');
      audioElement.play().then(() => {
        console.log('ðŸŽµ âœ… Audio element play() succeeded');
      }).catch((error) => {
        console.error('ðŸŽµ âŒ Audio element play() failed:', error.name, error.message);
        
        // iOS Safari often requires user interaction before playing audio
        if (isIOS) {
          console.warn('ðŸŽµ iOS detected - audio playback requires user interaction');
          console.warn('ðŸŽµ Audio will start playing after user taps/clicks anywhere on the screen');
        }
      });
    };

    // Setup global interaction listener for iOS (plays all audio elements on first touch)
    if (isIOS && !window.__audioInteractionHandlerSet) {
      window.__audioInteractionHandlerSet = true;
      
      // Show prompt for listeners on iOS
      if (!session?.isHost) {
        setShowIOSAudioPrompt(true);
      }
      
      const playAllAudioOnInteraction = () => {
        console.log('ðŸŽµ iOS: User interaction detected, playing all audio elements...');
        setShowIOSAudioPrompt(false); // Hide prompt after interaction
        
        const remoteAudios = document.getElementById('remote-audios');
        if (remoteAudios) {
          Array.from(remoteAudios.querySelectorAll('audio')).forEach(audio => {
            if (audio.paused) {
              audio.play().then(() => {
                console.log('ðŸŽµ âœ… Started audio element on interaction');
              }).catch(err => {
                console.error('ðŸŽµ âŒ Failed to start audio:', err.message);
              });
            }
          });
        }
      };
      document.addEventListener('touchstart', playAllAudioOnInteraction, { once: true });
      document.addEventListener('click', playAllAudioOnInteraction, { once: true });
    }

    // Small delay to ensure stream is ready (especially important on iOS)
    setTimeout(attemptPlay, 100);

    // Store the audio element
    const remoteAudios = document.getElementById('remote-audios');
    if (remoteAudios) {
      // Remove any existing audio elements for this stream
      const existingAudios = remoteAudios.querySelectorAll('audio');
      existingAudios.forEach(audio => {
        if (audio.srcObject === stream) {
          console.log('ðŸŽµ Removing existing audio element');
          audio.remove();
        }
      });

      console.log('ðŸŽµ Appending new audio element to DOM');
      remoteAudios.appendChild(audioElement);
      console.log('ðŸŽµ Total audio elements in container:', remoteAudios.children.length);
    } else {
      console.warn('ðŸŽµ Remote audios container not found');
    }
  };
  
  // Create and send offer
  const createAndSendOffer = async (signaling) => {
    try {
      const peerConnection = rtcConnectionRef.current;
      if (!peerConnection) {
        console.error('âŒ No peer connection available');
        return;
      }

      console.log('ðŸ“¤ Creating offer...');
      console.log('ðŸ“¤ Peer connection state:', {
        connectionState: peerConnection.connectionState,
        iceConnectionState: peerConnection.iceConnectionState,
        signalingState: peerConnection.signalingState,
        senders: peerConnection.getSenders().length
      });
      
      const offer = await peerConnection.createOffer();
      console.log('ðŸ“¤ Created offer:', offer.type);
      console.log('ðŸ“¤ Setting local description...');
      await peerConnection.setLocalDescription(offer);

      console.log('ðŸ“¤ Sending offer via signaling...');
      signaling.emit('webrtc-offer', {
        offer: peerConnection.localDescription,
        sessionId: sessionIdRef.current
      });
      console.log('ðŸ“¤ Offer sent successfully');
    } catch (error) {
      console.error('âŒ Error creating/sending offer:', error.message);
    }
  };

  // Attempt to publish using mediasoup handshake; graceful fallback to native
  const startMediasoupPublish = async (signaling) => {
    if (!signaling) throw new Error('No signaling available');
    if (!localStream) throw new Error('No local stream');

    // Request RTP capabilities from server
    const rtpCapsResp = await new Promise((res) => signaling.emit('mediasoup-get-rtpCapabilities', { sessionId }, (r) => res(r)));
    if (!rtpCapsResp || !rtpCapsResp.success) throw new Error('mediasoup unavailable or disabled');

    // Create a mediasoup Device
    const device = new mediasoupClient.Device();
    try {
      await device.load({ routerRtpCapabilities: rtpCapsResp.rtpCapabilities });
      mediasoupDeviceRef.current = device;
      console.log('âœ… Mediasoup device loaded with RTP capabilities');
    } catch (err) {
      console.warn('Failed to load mediasoup Device, falling back to native:', err.message);
      return nativePublishFallback(signaling);
    }

    // Create send transport for producing
    const sendTransportResp = await new Promise((res) => signaling.emit('mediasoup-create-transport', { sessionId }, (r) => res(r)));
    if (!sendTransportResp || !sendTransportResp.success) {
      console.warn('mediasoup create send transport failed, fallback to native');
      return nativePublishFallback(signaling);
    }

    const sendTransport = device.createSendTransport({
      id: sendTransportResp.transport.id,
      iceParameters: sendTransportResp.transport.iceParameters,
      iceCandidates: sendTransportResp.transport.iceCandidates,
      dtlsParameters: sendTransportResp.transport.dtlsParameters
    });

    sendTransport.on('connect', ({ dtlsParameters }, callback, errback) => {
      signaling.emit('mediasoup-connect-transport', { sessionId, transportId: sendTransportResp.transport.id, dtlsParameters }, (resp) => {
        if (resp && resp.success) callback(); else errback(resp && resp.error ? new Error(resp.error) : new Error('connect failed'));
      });
    });

    sendTransport.on('produce', async ({ kind, rtpParameters }, callback, errback) => {
      signaling.emit('mediasoup-produce', { sessionId, transportId: sendTransportResp.transport.id, kind, rtpParameters }, (resp) => {
        if (resp && resp.success) callback({ id: resp.id }); else errback(new Error(resp && resp.error ? resp.error : 'produce failed'));
      });
    });

    // Produce audio track
    const track = localStream.getAudioTracks()[0];
    if (!track) return nativePublishFallback(signaling);

    const producer = await sendTransport.produce({ track });
    console.log('ðŸŽ¤ Mediasoup producer created:', producer.id);

    // Create receive transport for consuming
    const recvTransportResp = await new Promise((res) => signaling.emit('mediasoup-create-transport', { sessionId }, (r) => res(r)));
    if (recvTransportResp && recvTransportResp.success) {
      const recvTransport = device.createRecvTransport({
        id: recvTransportResp.transport.id,
        iceParameters: recvTransportResp.transport.iceParameters,
        iceCandidates: recvTransportResp.transport.iceCandidates,
        dtlsParameters: recvTransportResp.transport.dtlsParameters
      });

      recvTransport.on('connect', ({ dtlsParameters }, callback, errback) => {
        signaling.emit('mediasoup-connect-transport', { sessionId, transportId: recvTransportResp.transport.id, dtlsParameters }, (resp) => {
          if (resp && resp.success) callback(); else errback(resp && resp.error ? new Error(resp.error) : new Error('connect failed'));
        });
      });

      recvTransportRef.current = recvTransport;
      console.log('âœ… Receive transport created for consuming');

      // Store our own producer ID to avoid consuming it
      const myProducerId = producer.id;

      // Get existing producers and consume them (skip our own)
      const existingProducersResp = await new Promise((res) => signaling.emit('mediasoup-get-producers', { sessionId }, (r) => res(r)));
      if (existingProducersResp && existingProducersResp.success && existingProducersResp.producerIds) {
        for (const producerId of existingProducersResp.producerIds) {
          if (producerId !== myProducerId) {
            await consumeProducer(signaling, producerId, recvTransport, device.rtpCapabilities);
          } else {
            console.log('â­ï¸ Skipping own producer:', producerId);
          }
        }
      }

      // Listen for new producers (skip our own)
      signaling.on('new-producer', async (data) => {
        if (data.producerId !== myProducerId) {
          console.log('ðŸ“¢ New producer available:', data.producerId);
          await consumeProducer(signaling, data.producerId, recvTransport, device.rtpCapabilities);
        } else {
          console.log('â­ï¸ Ignoring own producer event');
        }
      });
    }

    return { mode: 'mediasoup', producerId: producer.id };
  };

  // Consume a producer (receive audio from another participant)
  const consumeProducer = async (signaling, producerId, recvTransport, rtpCapabilities) => {
    try {
      const consumeResp = await new Promise((res) => 
        signaling.emit('mediasoup-consume', { 
          sessionId, 
          transportId: recvTransport.id, 
          producerId,
          rtpCapabilities 
        }, (r) => res(r))
      );

      if (!consumeResp || !consumeResp.success) {
        console.warn('Failed to consume producer:', producerId);
        return;
      }

      const consumer = await recvTransport.consume({
        id: consumeResp.consumer.id,
        producerId: consumeResp.consumer.producerId,
        kind: consumeResp.consumer.kind,
        rtpParameters: consumeResp.consumer.rtpParameters
      });

      // Resume the consumer to start receiving media
      await consumer.resume();
      console.log('â–¶ï¸ Consumer resumed:', consumer.id);

      consumersRef.current.set(consumer.id, consumer);
      console.log('ðŸŽ§ Consumer created:', consumer.id, 'for producer:', producerId);

      // Create audio element for the consumed track
      const stream = new MediaStream([consumer.track]);
      handleRemoteAudioStream(stream);

    } catch (err) {
      console.error('Error consuming producer:', producerId, err);
    }
  };

  // Native peer connection publish fallback: create offer and send via socket.io
  const nativePublishFallback = async (signaling) => {
    if (!rtcConnectionRef.current) await setupPeerConnection(signaling);
    const pc = rtcConnectionRef.current;
    if (!pc) throw new Error('No RTCPeerConnection available');

    // Ensure tracks are added
    if (localStream) {
      localStream.getTracks().forEach(track => pc.addTrack(track, localStream));
    }

    const offer = await pc.createOffer();
    await pc.setLocalDescription(offer);

    return new Promise((resolve, reject) => {
      signaling.emit('webrtc-offer', { offer: pc.localDescription, sessionId }, (ack) => {
        // ack might be undefined depending on server handlers; resolve anyway
        resolve(ack || { success: true });
      });
    });
  };

  // WebRTC signaling handlers
  const handleOffer = async (data, signaling) => {
    try {
      const peerConnection = rtcConnectionRef.current;
      if (!peerConnection) {
        console.error('âŒ No peer connection available for offer');
        return;
      }

      console.log('ðŸ“¥ Handling offer...');
      await peerConnection.setRemoteDescription(new RTCSessionDescription(data.offer));

      console.log('ðŸ“¥ Creating answer...');
      const answer = await peerConnection.createAnswer();
      console.log('ðŸ“¥ Setting local description...');
      await peerConnection.setLocalDescription(answer);

      console.log('ðŸ“¥ Sending answer...');
      signaling.emit('webrtc-answer', {
        answer: peerConnection.localDescription,
        sessionId: sessionIdRef.current
      });
      console.log('ðŸ“¥ Answer sent');
    } catch (error) {
      console.error('âŒ Error handling offer:', error);
    }
  };

  const handleAnswer = async (data, signaling) => {
    try {
      const peerConnection = rtcConnectionRef.current;
      if (!peerConnection) {
        console.error('âŒ No peer connection available for answer');
        return;
      }

      console.log('ðŸ“¥ Handling answer...');
      await peerConnection.setRemoteDescription(new RTCSessionDescription(data.answer));
      console.log('ðŸ“¥ Remote description set');
    } catch (error) {
      console.error('âŒ Error handling answer:', error);
    }
  };

  const handleCandidate = async (data, signaling) => {
    try {
      const peerConnection = rtcConnectionRef.current;
      if (!peerConnection) {
        console.error('âŒ No peer connection available for candidate');
        return;
      }

      console.log('ðŸ“¥ Adding ICE candidate...');
      await peerConnection.addIceCandidate(new RTCIceCandidate(data.candidate));
      console.log('ðŸ“¥ ICE candidate added');
    } catch (error) {
      console.error('âŒ Error handling ICE candidate:', error);
    }
  };
  
  const handleParticipantJoined = (participant) => {
    console.log('Participant joined:', participant);
    setParticipants(prev => [...prev, participant]);

    // If we have a peer connection ready and this isn't us joining, initiate connection
    if (rtcConnectionRef.current && signalingRef.current && participant.id !== user?.id) {
      console.log('Initiating WebRTC connection with new participant');
      // Send ready signal to trigger connection establishment
      signalingRef.current.emit('webrtc-ready', { sessionId });
    }
  };
  
  const handleParticipantLeft = (participantId) => {
    setParticipants(prev => prev.filter(p => p.id !== participantId));
  };
  
  // Leave session
  const leaveSession = async () => {
    try {
      if (session) {
        await api.post(`/audio/sessions/${session.id}/leave`, {
          user_id: user.id
        });
      }
      
      // Close WebRTC connection
      if (rtcConnectionRef.current) {
        rtcConnectionRef.current.close();
      }
      
      // Close signaling connection
      if (signalingRef.current) {
        // Use actual audio session ID (from session.id), NOT groupId
        signalingRef.current.emit('leave-audio-session', { sessionId: session?.id });
        signalingRef.current.disconnect();
      }
      
    } catch (error) {
      console.error('Error leaving session:', error);
    }
  };
  
  // Render component
  // Lightweight render-time log to aid debugging in desktop browsers (keeps console output concise)
  try {
    // Only log in non-production to avoid noisy output in production builds
    if (import.meta.env && import.meta.env.MODE !== 'production') {
      console.log('AudioSession render', {
        loading,
        error: !!error,
        audioError: !!audioError,
        session: !!session,
        isJoined,
        micInitialized,
        requiresUserGesture,
        micInitializing,
        localStream: !!localStream,
        connected,
        connecting
      });
    }
  } catch (e) {
    // Swallow any render-time logging errors to avoid breaking render
  }

  // Defensive checks for required dependencies
  if (typeof Container === 'undefined' || typeof Typography === 'undefined') {
    return (
      <div style={{ padding: '20px', color: 'red' }}>
        <h4>Render Error</h4>
        <p>Material-UI components not available</p>
        <button onClick={() => window.location.reload()}>Reload Page</button>
      </div>
    );
  }

  // Ensure critical state variables are properly initialized
  const safeParticipants = Array.isArray(participants) ? participants : [];
  const safeSession = session || {};
  const safeUser = user || {};

  return (
      <Container maxWidth="md" sx={{ position: 'relative' }}>
        {/* Traffic Jam App Bar - Lime Green for Voice */}
        <AppBar position="static" sx={{ bgcolor: '#76ff03', color: '#000' }}>
          <Toolbar>
            <IconButton edge="start" sx={{ color: '#000' }} onClick={handleLeaveAudio}>
              <ArrowBackIcon />
            </IconButton>
            <VolumeUpIcon sx={{ ml: 1, mr: 1, color: '#000' }} />
            <Typography variant="h6" sx={{ flexGrow: 1, color: '#000' }}>
              Voice
            </Typography>
            <IconButton sx={{ color: '#000' }} onClick={() => setOpenMusicDialog(true)}>
              <MusicNoteIcon />
            </IconButton>
            <IconButton sx={{ color: '#000' }} onClick={() => setOpenLeaveDialog(true)}>
              <LeaveIcon />
            </IconButton>
          </Toolbar>
        </AppBar>

        {/* Vertical Group Name Panel - Chartreuse/Olive Green */}
        <Box
          sx={{
            position: 'absolute',
            left: 0,
            top: 64, // Below AppBar
            height: 'calc(100vh - 64px)', // Full viewport height minus AppBar
            zIndex: 10,
            bgcolor: '#7CB342', // Chartreuse/olive green for Voice
            backdropFilter: 'blur(2px)',
            padding: '16px 4px',
            display: 'flex',
            flexDirection: 'column',
            alignItems: 'flex-end',
            justifyContent: 'flex-start',
            boxShadow: 2,
          }}
        >
          <Typography
            variant="body2"
            sx={{
              writingMode: 'vertical-rl',
              textOrientation: 'mixed',
              transform: 'rotate(180deg)',
              color: '#fff',
              fontWeight: 'bold',
              fontStyle: 'italic',
              letterSpacing: '0.1em',
            }}
          >
            {group?.name || 'Voice Session'}
          </Typography>
        </Box>

        {/* iOS Audio Prompt for Listeners */}
        {showIOSAudioPrompt && (
          <Alert 
            severity="info" 
            sx={{ mb: 2, cursor: 'pointer' }}
            onClick={() => {
              setShowIOSAudioPrompt(false);
              // Trigger audio play on all elements
              const remoteAudios = document.getElementById('remote-audios');
              if (remoteAudios) {
                Array.from(remoteAudios.querySelectorAll('audio')).forEach(audio => {
                  audio.play().catch(err => console.error('Audio play error:', err));
                });
              }
            }}
          >
            <strong>Tap here to enable audio</strong> - iOS requires user interaction to play audio
          </Alert>
        )}
      
      <Paper elevation={3} sx={{ p: 3, mb: 4 }}>
        {error && (
          <Alert severity="error" sx={{ mb: 2 }}>
            {error}
          </Alert>
        )}
        
        {audioError && (
          <Alert severity="error" sx={{ mb: 2 }}>
            {audioError}
          </Alert>
        )}
        
        {/* Only show Leave button */}
        <Box sx={{ display: 'flex', justifyContent: 'flex-end', mb: 3 }}>
          <Button 
            variant="outlined" 
            color="secondary" 
            onClick={handleLeaveAudio}
            disabled={!isJoined}
            sx={{ mr: 2 }}
          >
            Leave Audio
          </Button>

          {/* Debug button */}
          <Button
            variant="outlined"
            color="info"
            onClick={() => {
              console.log('ðŸ” DEBUG: Current WebRTC state');
              console.log('ðŸ” Peer connection:', rtcConnectionRef.current);
              if (rtcConnectionRef.current) {
                console.log('ðŸ” Connection state:', rtcConnectionRef.current.connectionState);
                console.log('ðŸ” ICE connection state:', rtcConnectionRef.current.iceConnectionState);
                console.log('ðŸ” Signaling state:', rtcConnectionRef.current.signalingState);
                console.log('ðŸ” Local description:', rtcConnectionRef.current.localDescription);
                console.log('ðŸ” Remote description:', rtcConnectionRef.current.remoteDescription);
              }
              console.log('ðŸ” Local stream:', localStream);
              if (localStream) {
                console.log('ðŸ” Local tracks:', localStream.getTracks().map(t => ({ kind: t.kind, enabled: t.enabled, readyState: t.readyState })));
              }
              const remoteAudios = document.getElementById('remote-audios');
              if (remoteAudios) {
                console.log('ðŸ” Remote audio elements:', remoteAudios.children.length);
                Array.from(remoteAudios.children).forEach((audio, i) => {
                  console.log(`ðŸ” Audio element ${i}:`, {
                    srcObject: !!audio.srcObject,
                    volume: audio.volume,
                    muted: audio.muted,
                    paused: audio.paused,
                    readyState: audio.readyState,
                    networkState: audio.networkState
                  });
                });
              }
              console.log('ðŸ” Signaling connected:', !!signalingRef.current);
              console.log('ðŸ” Participants:', participants);
            }}
            sx={{ mr: 2 }}
          >
            Debug State
          </Button>

          {connected ? (
            <Button variant="contained" color="error" onClick={() => {
              // Disconnect signaling
              if (signalingRef.current) {
                // Use actual audio session ID (from session.id), NOT groupId
                signalingRef.current.emit('leave-audio-session', { sessionId: safeSession?.id });
                signalingRef.current.disconnect();
                setSocket(null);
                signalingRef.current = null;
                setConnected(false);
              }
            }}>
              Disconnect Signaling
            </Button>
          ) : (
            <Button 
              variant="contained" 
              color="primary" 
              onClick={() => {
                try {
                  console.log('ðŸ”˜ Connect Signaling button clicked');
                  setConnecting(true);
                  setError(null); // Clear any previous errors
                  
                  // Connect signaling - use actual audio session ID (NOT groupId)
                  // groupId is for identifying the group, session.id is the MongoDB session document
                  const audioSessionId = safeSession?.id;
                  if (!audioSessionId) {
                    throw new Error('No audio session ID available');
                  }
                  
                  const s = setupSignaling(audioSessionId);
                  console.log('ðŸ“¡ Signaling setup initiated for session:', audioSessionId, 'groupId:', groupId);
                  
                  // once connected, try to initialize WebRTC
                  s.on('connect', () => {
                    console.log('ðŸŽ¯ Socket connected, initializing WebRTC...');
                    initializeWebRTC(audioSessionId);
                  });
                } catch (err) {
                  console.error('âŒ Error setting up signaling:', err);
                  setConnecting(false);
                  setError(`Failed to setup signaling: ${err.message}`);
                }
              }}
              disabled={!safeSession || !safeSession.id || connecting}
            >
              {connecting ? 'Connecting...' : 'Connect Signaling'}
            </Button>
          )}
        </Box>
        
        {/* Show microphone initialization prompt if needed */}
        {!micInitialized && (
          <Paper variant="outlined" sx={{ p: 3, mb: 2, bgcolor: 'info.light' }}>
            {console.log('ðŸŽ¤ Mic not initialized:', { micInitialized, requiresUserGesture, micInitializing, isIOS })}
            <Box sx={{ display: 'flex', alignItems: 'center', mb: 2 }}>
              <MicIcon sx={{ fontSize: 36, color: 'primary.main', mr: 2 }} />
              <Box sx={{ flexGrow: 1 }}>
                <Typography variant="h6">
                  Microphone Not Initialized
                </Typography>
                <Typography variant="body2" color="text.secondary">
                  {requiresUserGesture ?
                    'Click below to grant microphone access. Required for voice communication.' :
                    'Initializing microphone...'
                  }
                </Typography>
              </Box>
            </Box>
            {requiresUserGesture ? (
              <Button
                variant="contained"
                color="primary"
                size="large"
                fullWidth
                startIcon={<MicIcon />}
                onClick={() => {
                  console.log('ðŸŽ¤ User clicked Grant Microphone Access button');
                  console.log('ðŸŽ¤ Current state:', { micInitialized, micInitializing, requiresUserGesture });
                  initializeMicrophone();
                }}
                disabled={micInitializing}
              >
                {micInitializing ? (
                  <>
                    <CircularProgress size={20} sx={{ mr: 1 }} />
                    Initializing...
                  </>
                ) : (
                  'Grant Microphone Access'
                )}
              </Button>
            ) : (
              <Box sx={{ display: 'flex', justifyContent: 'center', alignItems: 'center', gap: 2 }}>
                <CircularProgress size={24} />
                <Typography variant="body2">Requesting permission...</Typography>
              </Box>
            )}
            {audioError && (
              <Alert severity="error" sx={{ mt: 2 }}>
                {audioError}
                <Button
                  variant="outlined"
                  size="small"
                  color="error"
                  onClick={() => {
                    console.log('ðŸŽ¤ User clicked Retry button');
                    setAudioError(null);
                    initializeMicrophone();
                  }}
                  disabled={micInitializing}
                  sx={{ mt: 1, ml: 2 }}
                >
                  Retry
                </Button>
              </Alert>
            )}
          </Paper>
        )}

        {/* Always show UI sections - just disable controls if mic not initialized */}
        <Box>
          {/* Simplified Mic controls */}
          <Paper variant="outlined" sx={{ p: 2, mb: 2, opacity: micInitialized ? 1 : 0.6 }}>
            <Typography variant="subtitle1" gutterBottom>
              Mic Controls {!micInitialized && <Chip label="Mic Not Ready" size="small" color="warning" sx={{ ml: 1 }} />}
            </Typography>
              
            <Grid container spacing={2} alignItems="center">
              <Grid item>
                <Tooltip title={
                  !micInitialized
                    ? "Microphone not initialized - click Grant Microphone Access above"
                    : isPushToTalkActive 
                      ? "Push-to-Talk Active" 
                      : isMuted 
                        ? "Unmute Microphone (tap) or Push-to-Talk (hold)" 
                        : "Mute Microphone"
                }>
                  <IconButton 
                    color={isPushToTalkActive ? "secondary" : isMuted ? "default" : "primary"} 
                    onMouseDown={handleMicButtonMouseDown}
                    onMouseUp={handleMicButtonMouseUp}
                    onMouseLeave={handleMicButtonMouseLeave}
                    onTouchStart={handleMicButtonMouseDown}
                    onTouchEnd={handleMicButtonMouseUp}
                    aria-label={isMuted ? "Unmute" : "Mute"}
                    disabled={!micInitialized}
                    sx={{ 
                      width: 56, 
                      height: 56,
                      transition: 'all 0.2s ease-in-out',
                      transform: isPushToTalkActive ? 'scale(1.2)' : 'scale(1)',
                      boxShadow: isPushToTalkActive ? '0 0 10px rgba(255,0,0,0.5)' : 'none'
                    }}
                  >
                    {isPushToTalkActive ? <MicIcon /> : isMuted ? <MicOffIcon /> : <MicIcon />}
                  </IconButton>
                </Tooltip>
              </Grid>
                
                <Grid item xs>
                  <Box sx={{ width: '100%' }}>
                    <Box sx={{ 
                      height: 8, 
                      bgcolor: 'grey.200', 
                      borderRadius: 1, 
                      overflow: 'hidden' 
                    }}>
                      <Box sx={{ 
                        width: `${inputLevel * 100}%`, 
                        height: '100%', 
                        bgcolor: inputLevel > 0.5 ? 'success.main' : 'primary.main',
                        transition: 'width 0.1s ease-in-out'
                      }} />
                    </Box>
                  </Box>
                </Grid>
                
                <Grid item>
                  <Typography variant="caption" color="textSecondary">
                    {isPushToTalkActive 
                      ? "Speaking (Push-to-Talk)" 
                      : isMuted 
                        ? "Muted (Tap to unmute, hold for Push-to-Talk)" 
                        : "Unmuted (Tap to mute)"}
                  </Typography>
                </Grid>
              </Grid>
            </Paper>
            
            {/* Speaker Controls */}
            <Paper variant="outlined" sx={{ p: 2 }}>
              <Typography variant="subtitle1" gutterBottom>
                Speaker Settings
              </Typography>

              {/* Local Audio Monitoring Toggle */}
              <Box sx={{ mb: 3 }}>
                <FormControlLabel
                  control={
                    <Switch
                      checked={localAudioMonitoring}
                      onChange={(e) => setLocalAudioMonitoring(e.target.checked)}
                      color="primary"
                    />
                  }
                  label="Monitor Local Audio (hear yourself)"
                />
                <Typography variant="caption" color="textSecondary">
                  Enable to test microphone input (low volume to avoid feedback)
                </Typography>
              </Box>
              
             {/* Volume Controls - Simplified for mobile-first browser experience */}
              {/* In native app, full volume controls are available. In browser, use device volume buttons */}
              <Box sx={{ mb: 3, width: '100%' }}>
                {/* Voice Mute Control */}
                <Box sx={{ 
                  display: 'flex', 
                  alignItems: 'center', 
                  justifyContent: 'space-between',
                  p: 2,
                  border: '1px solid',
                  borderColor: 'divider',
                  borderRadius: 1,
                  mb: 2,
                  bgcolor: isVoiceMuted ? 'error.50' : 'background.paper'
                }}>
                  <Box>
                    <Typography variant="subtitle1" fontWeight="600">
                      Voice Audio
                    </Typography>
                    <Typography variant="caption" color="text.secondary">
                      {isVoiceMuted ? 'Muted - Click to unmute' : 'Active - Click to mute'}
                    </Typography>
                  </Box>
                  <IconButton 
                    onClick={toggleVoiceMute}
                    color={isVoiceMuted ? "error" : "primary"}
                    size="large"
                    aria-label={isVoiceMuted ? "Unmute voice" : "Mute voice"}
                    sx={{ 
                      bgcolor: isVoiceMuted ? 'error.main' : 'primary.main',
                      color: 'white',
                      '&:hover': {
                        bgcolor: isVoiceMuted ? 'error.dark' : 'primary.dark'
                      }
                    }}
                  >
                    {isVoiceMuted ? <VolumeOffIcon /> : <VolumeUpIcon />}
                  </IconButton>
                </Box>

                {/* Music Mute Control */}
                <Box sx={{ 
                  display: 'flex', 
                  alignItems: 'center', 
                  justifyContent: 'space-between',
                  p: 2,
                  border: '1px solid',
                  borderColor: 'divider',
                  borderRadius: 1,
                  bgcolor: isMusicMuted ? 'error.50' : 'background.paper'
                }}>
                  <Box>
                    <Typography variant="subtitle1" fontWeight="600">
                      Music Audio
                    </Typography>
                    <Typography variant="caption" color="text.secondary">
                      {isMusicMuted ? 'Muted - Click to unmute' : 'Active - Click to mute'}
                    </Typography>
                  </Box>
                  <IconButton 
                    onClick={toggleMusicMute}
                    color={isMusicMuted ? "error" : "primary"}
                    size="large"
                    aria-label={isMusicMuted ? "Unmute music" : "Mute music"}
                    sx={{ 
                      bgcolor: isMusicMuted ? 'error.main' : 'primary.main',
                      color: 'white',
                      '&:hover': {
                        bgcolor: isMusicMuted ? 'error.dark' : 'primary.dark'
                      }
                    }}
                  >
                    {isMusicMuted ? <VolumeOffIcon /> : <VolumeUpIcon />}
                  </IconButton>
                </Box>

                {/* Volume info message */}
                <Typography variant="caption" color="info.main" sx={{ display: 'block', mt: 2, textAlign: 'center' }}>
                  ðŸ’¡ Use your device volume buttons to adjust audio levels
                </Typography>
              </Box>
            </Paper>

            {/* Hidden container for remote audio elements */}
            <div id="remote-audios" style={{ display: 'none' }}></div>

            {/* Local audio monitoring for testing */}
            {localStream && localAudioMonitoring && (
              <audio
                ref={(audio) => {
                  if (audio && localStream) {
                    audio.srcObject = localStream;
                    audio.volume = 0.5; // Increased volume for testing
                    audio.muted = false;
                  }
                }}
                style={{ display: 'none' }}
                autoPlay
                muted={false}
              />
            )}

            {/* Simple Audio Test */}
            <Paper variant="outlined" sx={{ mt: 2, p: 2 }}>
              <Typography variant="subtitle1" gutterBottom>
                Audio Test
              </Typography>
              <Typography variant="body2" color="text.secondary" sx={{ mb: 2 }}>
                Test your microphone by speaking. The level meter above should move when you talk.
              </Typography>
              <Box sx={{ display: 'flex', gap: 2, alignItems: 'center' }}>
                <Button
                  variant="outlined"
                  onClick={() => {
                    if (localStream) {
                      console.log('ðŸŽ¤ Current audio tracks:', localStream.getAudioTracks().map(t => ({
                        enabled: t.enabled,
                        muted: t.muted,
                        readyState: t.readyState,
                        label: t.label
                      })));
                      alert(`Microphone working! Tracks: ${localStream.getAudioTracks().length}`);
                    } else {
                      alert('No microphone stream available');
                    }
                  }}
                >
                  Test Mic Status
                </Button>
                <Typography variant="caption" color="text.secondary">
                  Input Level: {Math.round(inputLevel * 100)}%
                </Typography>
              </Box>
            </Paper>
            <Paper variant="outlined" sx={{ mt: 2, p: 2 }}>
              <Box sx={{ display: 'flex', alignItems: 'center', justifyContent: 'space-between', mb: 1 }}>
                <Typography variant="subtitle1">Participants ({safeParticipants.length})</Typography>
                {connected ? (
                  <Chip 
                    label="Connected" 
                    size="small" 
                    color="success" 
                    sx={{ fontWeight: 'bold' }} 
                  />
                ) : connecting ? (
                  <Chip 
                    label="Connecting..." 
                    size="small" 
                    color="warning" 
                  />
                ) : (
                  <Chip 
                    label="Disconnected" 
                    size="small" 
                    color="error" 
                  />
                )}
              </Box>
              <List dense>
                {safeParticipants && safeParticipants.length > 0 ? safeParticipants
                  .filter(p => p) // Filter out any null/undefined participants
                  .map((p, idx) => {
                    // Determine avatar source
                    const avatarSrc = p.profile_image_url || p.avatar || null;
                    const initials = p.display_name ? p.display_name.split(' ').map(n => n[0]).join('').substring(0, 2).toUpperCase() : '?';
                    
                    return (
                      <React.Fragment key={p.socketId || p.id || `participant-${idx}`}>
                        <ListItem>
                          <ListItemAvatar>
                            <Avatar src={avatarSrc} alt={p.display_name}>
                              {!avatarSrc && initials}
                            </Avatar>
                          </ListItemAvatar>
                          <ListItemText 
                            primary={
                              <Box sx={{ display: 'flex', alignItems: 'center', gap: 1 }}>
                                <Typography variant="body2">{p.display_name || 'Unknown'}</Typography>
                                {p.isMe && <Chip label="You" size="small" color="primary" />}
                              </Box>
                            }
                            secondary={
                              <Typography variant="caption" color="text.secondary">
                                {p.isMuted ? 'ðŸ”‡ Muted' : 'ðŸŽ¤ Active'}
                              </Typography>
                            }
                          />
                        </ListItem>
                        {idx < safeParticipants.length - 1 && <Divider variant="inset" component="li" />}
                      </React.Fragment>
                    );
                  }) : (
                  <ListItem>
                    <ListItemText primary="No participants yet" secondary="Waiting for others to join..." />
                  </ListItem>
                )}
              </List>
            </Paper>
        </Box>
      </Paper>
      
      {/* Music Dialog */}
      <Dialog
        open={openMusicDialog}
        onClose={() => setOpenMusicDialog(false)}
        maxWidth="md"
        fullWidth
        sx={{ zIndex: 9999 }}
      >
        <DialogTitle>Music Player</DialogTitle>
        <DialogContent>
          {/* Music Player */}
          <Box sx={{ mb: 3 }}>
            <MusicPlayer
              currentTrack={currentTrack}
              isPlaying={musicIsPlaying}
              currentTime={musicCurrentTime}
              duration={musicDuration}
              volume={musicVolume}
              isController={isMusicController}
              onPlay={musicPlay}
              onPause={musicPause}
              onNext={musicNext}
              onPrevious={musicPrevious}
              onSeek={musicSeek}
              onVolumeChange={changeMusicVolume}
              onTakeControl={takeMusicControl}
              onReleaseControl={releaseMusicControl}
              disabled={!session}
            />
          </Box>

          {/* Music Playlist */}
          <Box sx={{ mb: 3 }}>
            <Typography variant="subtitle1" gutterBottom>
              Playlist
            </Typography>
            <MusicPlaylist
              playlist={playlist}
              currentTrack={currentTrack}
              isController={isMusicController}
              onPlayTrack={(track) => musicLoadAndPlay(track)}
              onRemoveTrack={(trackId) => musicRemoveTrack(trackId)}
              disabled={!session}
            />
          </Box>

          {/* Music Upload */}
          <Box>
            <Typography variant="subtitle1" gutterBottom>
              Upload Music
            </Typography>
            <MusicUpload
              sessionId={session?.id || session?._id}
              onTracksAdded={async (tracks) => {
                console.log('Tracks uploaded:', tracks);
                // Add each track to the playlist
                for (const track of tracks) {
                  await musicAddTrack(track);
                }
                console.log('All tracks added to playlist');
              }}
              disabled={!session}
            />
          </Box>
        </DialogContent>
        <DialogActions>
          <Button onClick={() => setOpenMusicDialog(false)}>
            Close
          </Button>
        </DialogActions>
      </Dialog>

      {/* Leave Dialog */}
      <Dialog
        open={openLeaveDialog}
        onClose={() => setOpenLeaveDialog(false)}
      >
        <DialogTitle>Leave Traffic Jam Session?</DialogTitle>
        <DialogContent>
          <Typography>
            Are you sure you want to leave this audio session? You will need to rejoin to continue communicating.
          </Typography>
        </DialogContent>
        <DialogActions>
          <Button onClick={() => setOpenLeaveDialog(false)}>
            Cancel
          </Button>
          <Button 
            onClick={() => {
              setOpenLeaveDialog(false);
              handleLeaveAudio();
            }} 
            color="secondary"
            disabled={leaveLoading}
          >
            {leaveLoading ? <CircularProgress size={24} /> : 'Leave'}
          </Button>
        </DialogActions>
      </Dialog>
    </Container>
  );
};

export default AudioSession;
